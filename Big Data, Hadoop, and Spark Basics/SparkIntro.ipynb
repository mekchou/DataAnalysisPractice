{"cells":[{"cell_type":"markdown","id":"e7cbc73c-5aca-4ae4-ac2c-ba4f997e9aee","metadata":{},"source":["# **Getting Started With Spark using Python**\n"]},{"cell_type":"markdown","id":"90b789df-656d-4dc7-8eda-a4dbdece78cd","metadata":{},"source":["Estimated time needed: **15** minutes\n"]},{"cell_type":"markdown","id":"c0fca1c7-bc68-42ed-abeb-6a3efcc2a8a6","metadata":{},"source":["![](http://spark.apache.org/images/spark-logo.png)\n"]},{"cell_type":"markdown","id":"a045eb55-707b-47a7-a88d-21d97ac42f32","metadata":{},"source":["### The Python API\n"]},{"cell_type":"markdown","id":"96cf98d8-ebaa-4a01-8de6-864a1436d579","metadata":{},"source":["Spark is written in Scala, which compiles to Java bytecode, but you can write python code to communicate to the java virtual machine through a library called py4j. Python has the richest API, but it can be somewhat limiting if you need to use a method that is not available, or if you need to write a specialized piece of code. The latency associated with communicating back and forth to the JVM can sometimes cause the code to run slower.\n","An exception to this is the SparkSQL library, which has an execution planning engine that precompiles the queries. Even with this optimization, there are cases where the code may run slower than the native scala version.\n","The general recommendation for PySpark code is to use the \"out of the box\" methods available as much as possible and avoid overly frequent (iterative) calls to Spark methods. If you need to write high-performance or specialized code, try doing it in scala.\n","But hey, we know Python rules, and the plotting libraries are way better. So, it's up to you!\n"]},{"cell_type":"markdown","id":"ffa76ecd-c60d-4e43-9fb0-1da6e72c84c7","metadata":{},"source":["## How to run Jupyter notebook cell ?\n"]},{"cell_type":"markdown","id":"468ab580-cd76-4bff-8b31-72c04db5ba95","metadata":{},"source":["Jupyter Notebooks are a powerful way to write and iterate on your Python code for data analysis.The Jupyter Notebook is organized into cells. Each cell can contain code, text, or visual elements. \n","Rather than writing and re-writing an entire program, Jupyter Notebooks allow you to write code in separate blocks (or “cells”) and run each block of code individually. \n","Then, if you need to make a change, you can go back and make an edit and rerun the program again, all in the same window.\n","\n","To select the cell, simply click on it. The selected cell will be highlighted, indicating that it is ready for execution.Once you have the desired cell selected, you have multiple options to run it: \n","* Keyboard Shortcut: Press Shift + Enter on your keyboard to run the selected cell. This will execute the code or process the content within the cell and display the output below it. \n","If the cell has any output, it will be shown just after running.\n","* Toolbar: Look for the \"Run Cell\" button in the toolbar at the top of the Theia Lab interface. Clicking on this button will also execute the selected cell.\n","After running the cell, you can see the output displayed below it. If the cell contains any code that produces visual outputs, such as plots or images, they will be shown in the output area.\n","\n","To run subsequent cells, repeat the same steps: select the next cell you want to execute and use the appropriate method mentioned above.\n","Remember to execute cells in the correct order if there are dependencies between them.\n","For example,if you have variables defined in a previous cell that are needed in the current cell, make sure to run the preceding cell first.\n"]},{"cell_type":"markdown","id":"1f7b7581-f539-40cd-8874-5336e8959c41","metadata":{},"source":["## Objectives\n"]},{"cell_type":"markdown","id":"52d16272-d10a-45a8-9196-74404f795549","metadata":{},"source":["In this lab, we will go over the basics of Apache Spark and PySpark. We will start with creating the SparkContext and SparkSession. We then create an RDD and apply some basic transformations and actions. Finally we demonstrate the basics dataframes and SparkSQL.\n","\n","After this lab you will be able to:\n","\n","* Create the SparkContext and SparkSession\n","* Create an RDD and apply some basic transformations and actions to RDDs\n","* Demonstrate the use of the basics Dataframes and SparkSQL\n"]},{"cell_type":"markdown","id":"998acb19-2a7f-471d-b8bb-09762c374618","metadata":{},"source":["----\n"]},{"cell_type":"markdown","id":"3da7323c-6bc8-45f4-9dcb-3ddc12950d1a","metadata":{},"source":["## Setup\n"]},{"cell_type":"markdown","id":"2044ea79-6fd9-46bc-b0a2-889d83bebaf5","metadata":{},"source":["For this lab, we are going to be using Python and Spark (PySpark). These libraries should be installed in your lab environment or in SN Labs.\n"]},{"cell_type":"code","execution_count":null,"id":"c8943516-3586-4a46-a0e3-c3dc843c3847","metadata":{},"outputs":[],"source":["# Installing required packages\n","!pip install pyspark\n","!pip install findspark"]},{"cell_type":"code","execution_count":1,"id":"59e33b1e-6690-444f-b58e-7cc8542f9784","metadata":{},"outputs":[],"source":["import findspark\n","findspark.init()"]},{"cell_type":"code","execution_count":2,"id":"55ea8a24-6740-48bc-b291-ba777b46ad00","metadata":{},"outputs":[],"source":["# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the spark context. \n","from pyspark import SparkContext, SparkConf\n","from pyspark.sql import SparkSession"]},{"cell_type":"markdown","id":"04f39a74-42a2-4e27-a41a-def42563b80c","metadata":{},"source":["## Exercise 1 -  Spark Context and Spark Session\n"]},{"cell_type":"markdown","id":"d87fbe8c-72db-4368-8bb2-f80661a0eea2","metadata":{},"source":["In this exercise, you will create the Spark Context and initialize the Spark session needed for SparkSQL and DataFrames.\n","SparkContext is the entry point for Spark applications and contains functions to create RDDs such as `parallelize()`. SparkSession is needed for SparkSQL and DataFrame operations.\n"]},{"cell_type":"markdown","id":"0e0c2e65-875f-4e28-ab79-4bdd2e1a5e36","metadata":{},"source":["#### Task 1: Creating the spark session and context\n"]},{"cell_type":"code","execution_count":13,"id":"072d359e-ed4b-4519-b01a-cb246ff9a4a4","metadata":{},"outputs":[],"source":["# Creating a spark context class\n","\n","# Creating a spark session\n","    # .config(\"spark.some.config.option\", \"some-value\") \\\n","spark = SparkSession \\\n","    .builder \\\n","    .appName(\"Python Spark DataFrames basic example\") \\\n","    .config(\"spark.executor.memory\", \"2g\") \\\n","    .config(\"spark.executor.cores\", \"2\") \\\n","    .config(\"spark.driver.memory\", \"2g\") \\\n","    .getOrCreate()\n","sc = spark.sparkContext"]},{"cell_type":"markdown","id":"78e56646-d580-4a9c-a35d-e1a159eb9e42","metadata":{},"source":["#### Task 2: Initialize Spark session\n","To work with dataframes we just need to verify that the spark session instance has been created.\n"]},{"cell_type":"code","execution_count":14,"id":"af9de502-db3d-40a6-962f-5a3ba34faef7","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["SparkSession is active and ready to use.\n"]}],"source":["if 'spark' in locals() and isinstance(spark, SparkSession):\n","    print(\"SparkSession is active and ready to use.\")\n","else:\n","    print(\"SparkSession is not active. Please create a SparkSession.\")"]},{"cell_type":"markdown","id":"f3930200-0245-4775-b7d3-e72ee2725aa0","metadata":{},"source":["## Exercise 2: RDDs\n","In this exercise we work with Resilient Distributed Datasets (RDDs). RDDs are Spark's primitive data abstraction and we use concepts from functional programming to create and manipulate RDDs. \n"]},{"cell_type":"markdown","id":"1fc1a145-440f-454a-a4cd-c42b5ea3b075","metadata":{},"source":["#### Task 1: Create an RDD.\n","For demonstration purposes, we create an RDD here by calling `sc.parallelize()`  \n","We create an RDD which has integers from 1 to 30.\n"]},{"cell_type":"code","execution_count":15,"id":"87aa9903-569b-45e8-ac24-d0b66f423c48","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]},{"data":{"text/plain":["PythonRDD[4] at RDD at PythonRDD.scala:53"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["data = range(1,30)\n","# print first element of iterator\n","print(data[0])\n","len(data)\n","xrangeRDD = sc.parallelize(data, 4)\n","\n","# this will let us know that we created an RDD\n","xrangeRDD"]},{"cell_type":"markdown","id":"bf3d9add-4d94-4074-8c19-037840e58dc3","metadata":{},"source":["#### Task 2: Transformations\n"]},{"cell_type":"markdown","id":"43bbc5e3-3608-4bf8-888d-0081bfcc506a","metadata":{},"source":["A transformation is an operation on an RDD that results in a new RDD. The transformed RDD is generated rapidly because the new RDD is lazily evaluated, which means that the calculation is not carried out when the new RDD is generated. The RDD will contain a series of transformations, or computation instructions, that will only be carried out when an action is called. In this transformation, we reduce each element in the RDD by 1. Note the use of the lambda function. We also then filter the RDD to only contain elements <10.\n"]},{"cell_type":"code","execution_count":16,"id":"bd231b50-39e4-4d32-851f-e06867fb8f4f","metadata":{},"outputs":[],"source":["subRDD = xrangeRDD.map(lambda x: x-1)\n","filteredRDD = subRDD.filter(lambda x : x<10)\n"]},{"cell_type":"markdown","id":"1fbde2a6-70a1-4a5c-8405-fbd1301e723d","metadata":{},"source":["#### Task 3: Actions \n"]},{"cell_type":"markdown","id":"93873718-f8a9-482f-8eb6-ed13dd008793","metadata":{},"source":["A transformation returns a result to the driver. We now apply the `collect()` action to get the output from the transformation.\n"]},{"cell_type":"code","execution_count":18,"id":"b8149385-7d55-4d55-b159-8778e09bfeb5","metadata":{},"outputs":[{"ename":"Py4JJavaError","evalue":"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 1 times, most recent failure: Lost task 1.0 in stage 3.0 (TID 13) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# print(filteredRDD.collect())\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mfilteredRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mv:\\DataAnalysisPractice\\.venv\\Lib\\site-packages\\pyspark\\rdd.py:2316\u001b[0m, in \u001b[0;36mRDD.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   2296\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2297\u001b[0m \u001b[38;5;124;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[0;32m   2298\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2314\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   2315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mv:\\DataAnalysisPractice\\.venv\\Lib\\site-packages\\pyspark\\rdd.py:2291\u001b[0m, in \u001b[0;36mRDD.sum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[NumberOrArray]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumberOrArray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2272\u001b[0m \u001b[38;5;124;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[0;32m   2273\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2289\u001b[0m \u001b[38;5;124;03m    6.0\u001b[39;00m\n\u001b[0;32m   2290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[0;32m   2292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\n\u001b[0;32m   2293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mv:\\DataAnalysisPractice\\.venv\\Lib\\site-packages\\pyspark\\rdd.py:2044\u001b[0m, in \u001b[0;36mRDD.fold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m   2039\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[0;32m   2041\u001b[0m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[0;32m   2042\u001b[0m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[1;32m-> 2044\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n","File \u001b[1;32mv:\\DataAnalysisPractice\\.venv\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n","File \u001b[1;32mv:\\DataAnalysisPractice\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[1;32mv:\\DataAnalysisPractice\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n","File \u001b[1;32mv:\\DataAnalysisPractice\\.venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n","\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 1 times, most recent failure: Lost task 1.0 in stage 3.0 (TID 13) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n"]}],"source":["# print(filteredRDD.collect())\n","filteredRDD.count()"]},{"cell_type":"markdown","id":"00c9b539-c49e-4570-89b2-ba53b487c356","metadata":{},"source":["#### Task 4: Caching Data\n"]},{"cell_type":"markdown","id":"eaadcc62-1e5d-4160-81bc-5cdc6d1fb957","metadata":{},"source":["This simple example shows how to create an RDD and cache it. Notice the **10x speed improvement**!  If you wish to see the actual computation time, browse to the Spark UI...it's at host:4040.  You'll see that the second calculation took much less time!\n"]},{"cell_type":"code","execution_count":null,"id":"973abd3d-f044-4769-8118-5996cfc9da41","metadata":{},"outputs":[],"source":["import time \n","\n","test = sc.parallelize(range(1,50000),4)\n","test.cache()\n","\n","t1 = time.time()\n","# first count will trigger evaluation of count *and* cache\n","count1 = test.count()\n","dt1 = time.time() - t1\n","print(\"dt1: \", dt1)\n","\n","\n","t2 = time.time()\n","# second count operates on cached data only\n","count2 = test.count()\n","dt2 = time.time() - t2\n","print(\"dt2: \", dt2)\n","\n","#test.count()"]},{"cell_type":"markdown","id":"379be7f9-e20a-4dc1-b8f1-e01fd7362677","metadata":{},"source":["## Exercise 3: DataFrames and SparkSQL\n"]},{"cell_type":"markdown","id":"e2167e8f-93b3-4769-aacf-eefa7c60b49b","metadata":{},"source":["In order to work with the extremely powerful SQL engine in Apache Spark, you will need a Spark Session. We have created that in the first Exercise, let us verify that spark session is still active.\n"]},{"cell_type":"code","execution_count":19,"id":"4583068a-a298-4a86-8133-2df079b19b54","metadata":{},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Python Spark DataFrames basic example</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x265d753b350>"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"markdown","id":"7e89d7ba-15e9-4cac-8899-11bce38b7454","metadata":{},"source":["#### Task 1: Create Your First DataFrame!\n"]},{"cell_type":"markdown","id":"f84d8a41-ee26-417a-8212-b448a2b10ef2","metadata":{},"source":["You can create a structured data set (much like a database table) in Spark.  Once you have done that, you can then use powerful SQL tools to query and join your dataframes.\n"]},{"cell_type":"code","execution_count":null,"id":"ad6881aa-14b6-4ddd-ae4d-0f40228f34e1","metadata":{},"outputs":[],"source":["# Download the data first into a local `people.json` file\n","!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/people.json >> people.json"]},{"cell_type":"code","execution_count":20,"id":"444101eb-2f35-4361-b4e8-cf70bb4554ac","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["<>:2: SyntaxWarning: invalid escape sequence '\\D'\n","<>:2: SyntaxWarning: invalid escape sequence '\\D'\n","C:\\Users\\Mekchou\\AppData\\Local\\Temp\\ipykernel_3772\\2011915934.py:2: SyntaxWarning: invalid escape sequence '\\D'\n","  df = spark.read.json(\"V:\\DataAnalysisPractice\\data\\people.json\").cache()\n"]}],"source":["# Read the dataset into a spark dataframe using the `read.json()` function\n","df = spark.read.json(\"V:\\DataAnalysisPractice\\data\\people.json\").cache()"]},{"cell_type":"code","execution_count":21,"id":"b0390210-a8e4-4838-a629-1aee24c198e2","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-------+\n","| age|   name|\n","+----+-------+\n","|NULL|Michael|\n","|  30|   Andy|\n","|  19| Justin|\n","+----+-------+\n","\n","root\n"," |-- age: long (nullable = true)\n"," |-- name: string (nullable = true)\n","\n"]}],"source":["# Print the dataframe as well as the data schema\n","df.show()\n","df.printSchema()"]},{"cell_type":"code","execution_count":22,"id":"48265074-8f9e-4c88-a944-bbf0c8c31791","metadata":{},"outputs":[],"source":["# Register the DataFrame as a SQL temporary view\n","df.createTempView(\"people\")"]},{"cell_type":"markdown","id":"236eb733-cfec-4523-8946-e76ee3466053","metadata":{},"source":["#### Task 2: Explore the data using DataFrame functions and SparkSQL\n","\n","In this section, we explore the datasets using functions both from dataframes as well as corresponding SQL queries using sparksql. Note the different ways to achieve the same task!\n"]},{"cell_type":"code","execution_count":23,"id":"bce819a0-1eef-488d-9b99-1af0f546f09d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+\n","|   name|\n","+-------+\n","|Michael|\n","|   Andy|\n","| Justin|\n","+-------+\n","\n","+-------+\n","|   name|\n","+-------+\n","|Michael|\n","|   Andy|\n","| Justin|\n","+-------+\n","\n","+-------+\n","|   name|\n","+-------+\n","|Michael|\n","|   Andy|\n","| Justin|\n","+-------+\n","\n"]}],"source":["# Select and show basic data columns\n","\n","df.select(\"name\").show()\n","df.select(df[\"name\"]).show()\n","spark.sql(\"SELECT name FROM people\").show()"]},{"cell_type":"code","execution_count":24,"id":"32362f2a-f153-4999-97c5-1867b5931749","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----+\n","|age|name|\n","+---+----+\n","| 30|Andy|\n","+---+----+\n","\n","+---+----+\n","|age|name|\n","+---+----+\n","| 30|Andy|\n","+---+----+\n","\n"]}],"source":["# Perform basic filtering\n","\n","df.filter(df[\"age\"] > 21).show()\n","spark.sql(\"SELECT age, name FROM people WHERE age > 21\").show()"]},{"cell_type":"code","execution_count":25,"id":"86296424-2388-4a77-aac2-bd0e88c18b93","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+-----+\n","| age|count|\n","+----+-----+\n","|  19|    1|\n","|NULL|    1|\n","|  30|    1|\n","+----+-----+\n","\n","+----+-----+\n","| age|count|\n","+----+-----+\n","|  19|    1|\n","|NULL|    0|\n","|  30|    1|\n","+----+-----+\n","\n"]}],"source":["# Perfom basic aggregation of data\n","\n","df.groupBy(\"age\").count().show()\n","spark.sql(\"SELECT age, COUNT(age) as count FROM people GROUP BY age\").show()"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x00000265D753B350>>"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["spark.stop"]},{"cell_type":"markdown","id":"d315f26c-fb81-42a1-ba5e-896b223db5b2","metadata":{},"source":["----\n"]},{"cell_type":"markdown","id":"f3996484-5848-4733-92f1-f11e323d408f","metadata":{},"source":["### Question 1 - RDDs\n"]},{"cell_type":"markdown","id":"e1d10dcf-9638-4a02-9fb3-108bcdf6422b","metadata":{},"source":["Create an RDD with integers from 1-50. Apply a transformation to multiply every number by 2, resulting in an RDD that contains the first 50 even numbers. \n"]},{"cell_type":"code","execution_count":null,"id":"781d2a47-a456-467f-84e2-ee960b391c19","metadata":{},"outputs":[],"source":["# starter code\n","# numbers = range(1, 50)\n","# numbers_RDD = ...\n","# even_numbers_RDD = numbers_RDD.map(lambda x: ..)"]},{"cell_type":"code","execution_count":null,"id":"286d745e-a332-4161-8673-b98d9944aaf1","metadata":{},"outputs":[],"source":["# Code block for learners to answer"]},{"cell_type":"markdown","id":"70220c47-4bfa-453e-85ff-964467dcbe50","metadata":{},"source":["Double-click **here** for the solution.\n","\n","<!-- The answer is below:\n","numbers = range(1, 50) \n","numbers_RDD = sc.parallelize(numbers) \n","even_numbers_RDD = numbers_RDD.map(lambda x: x * 2)\n","print( even_numbers_RDD.collect()) \n","-->\n"]},{"cell_type":"markdown","id":"a9b48815-4670-4cf7-876c-2f600dba9788","metadata":{},"source":["### Question 2 - DataFrames and SparkSQL\n"]},{"cell_type":"markdown","id":"295642d6-c49e-4eb0-ab3a-659028111b90","metadata":{},"source":["Similar to the `people.json` file, now read the `people2.json` file into the notebook, load it into a dataframe and apply SQL operations to determine the average age in our people2 file.\n"]},{"cell_type":"code","execution_count":null,"id":"d69600b6-f151-4885-8270-a1c5ebb15eb2","metadata":{},"outputs":[],"source":["# starter code\n","# !curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/people2.json >> people2.json\n","# df = spark.read...\n","# df.createTempView..\n","# spark.sql(\"SELECT ...\")"]},{"cell_type":"code","execution_count":null,"id":"badac403-eda7-4d5a-9110-76c234fb6350","metadata":{},"outputs":[],"source":["# Code block for learners to answer"]},{"cell_type":"markdown","id":"7790162f-f410-4b11-aa19-7656bc92289d","metadata":{},"source":["Double-click **here** for a hint.\n","\n","<!-- The hint is below:\n","\n","1. The SQL query \"Select AVG(column_name) from..\" can be used to find the average value of a column. \n","2. Another possible way is to use the dataframe operations select() and mean()\n","-->\n"]},{"cell_type":"markdown","id":"225d9c83-12d7-4378-ac0d-dc9cf1cca6fa","metadata":{},"source":["Double-click **here** for the solution.\n","\n","<!-- The answer is below:\n","!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/people2.json >> people2.json\n","df = spark.read.json(\"people2.json\").cache()\n","df.createTempView(\"people2\")\n","result = spark.sql(\"SELECT AVG(age) from people2\")\n","result.show()\n","-->\n"]},{"cell_type":"markdown","id":"54315d30-8580-42e0-92c1-2df745d15d51","metadata":{},"source":["### Question 3 - SparkSession\n"]},{"cell_type":"markdown","id":"55bfd91e-f9d8-4b67-8f27-dae8c1c6a9ee","metadata":{},"source":["Close the SparkSession we created for this notebook\n"]},{"cell_type":"code","execution_count":null,"id":"a1e891bc-49f1-47d3-b68d-eb1ad6e274a3","metadata":{},"outputs":[],"source":["# Code block for learners to answer"]},{"cell_type":"markdown","id":"01069fc0-764d-42cd-a28e-e24f86af55e3","metadata":{},"source":["Double-click **here** for the solution.\n","\n","<!-- The answer is below:\n","\n","spark.stop() will stop the spark session\n","\n","-->\n"]},{"cell_type":"markdown","id":"4b6471c6-0439-4dd2-8ad1-4c051ed831ca","metadata":{},"source":["## Authors\n"]},{"cell_type":"markdown","id":"65cc90ae-4b42-4567-92d8-cd981a5df0d3","metadata":{},"source":["[Karthik Muthuraman](https://www.linkedin.com/in/karthik-muthuraman/)\n"]},{"cell_type":"markdown","id":"f4d74f61-76b3-4715-944f-6dbc74cb5586","metadata":{},"source":["### Other Contributors\n"]},{"cell_type":"markdown","id":"0c9d464f-1f42-4b9d-aa3d-5bb6fa553472","metadata":{},"source":["[Jerome Nilmeier](https://github.com/nilmeier)\n"]},{"cell_type":"markdown","id":"fdec7e13-ed6f-4187-823a-0ccf3b19f04f","metadata":{},"source":["<!--## Change Log -->\n"]},{"cell_type":"markdown","id":"2639a95b-6b90-49c5-9dd2-3d2d5ae88563","metadata":{},"source":["<!--|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2023-11-13|0.3|Ritika|Added Note|\n","|2021-07-02|0.2|Karthik|Beta launch|\n","|2021-06-30|0.1|Karthik|First Draft|-->\n"]},{"cell_type":"markdown","id":"d3d84813-b427-47cb-a903-856a595d3a63","metadata":{},"source":["<h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"prev_pub_hash":"e1ad62faa424d34e707cec17aeb0f9861646fd1eb6856d7c0047335c6ed4463f"},"nbformat":4,"nbformat_minor":4}
